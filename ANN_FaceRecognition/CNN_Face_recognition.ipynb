{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmp007/Machine-Learning-and-EDA-Exploratory-Data-Analysis-Projects/blob/main/ANN_FaceRecognition/CNN_Face_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdOnojsH2Yh2"
      },
      "source": [
        "<img src=\"https://cekumagroup.cas.lehigh.edu/sites/cekumagroup.cas2.lehigh.edu/files/image10.jpg \"/>\n",
        "\n",
        "# About the dataset \n",
        "## Facial Recognition with Deep Learning in Keras using CNN\n",
        "    \n",
        "# Problem Statement\n",
        "* Facial recognition is a biometric alternative that measures unique characteristics of a human face. Applications available today include flight check in, tagging friends and family members in photos, and \"tailored\" advertisement. You are a computer vision Engineer who needs to develop a face recognition programme with deep convolutional neural networks.\n",
        "\n",
        "# Objective \n",
        "* Use a deep convolutional neural network to perform facial recongition using Kera.\n",
        "\n",
        "\n",
        "*Notes*\n",
        "### Dataset Details:\n",
        "- ORL face database composed of 400 images of size 112 x 92. \n",
        "- There are 40 people, 10 images per person.\n",
        "- The images were taken at different times, lighting and facial expressions.\n",
        "- The faces are in an upright position in frontal view, with a slight left-right rotation.\n",
        "**Happy reading!**\n",
        "\n",
        "* * *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xIGh_gw2Yh-"
      },
      "source": [
        " # Step1:\n",
        " \n",
        "## Import the required library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9gOPmB1Z2Yh_"
      },
      "outputs": [],
      "source": [
        "#Basic libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Metric analysis libraries\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.utils import np_utils\n",
        "import itertools\n",
        "\n",
        "\n",
        "# ANN-related libraries\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "## This is expensive. Only turn on if you have GPU enabled\n",
        "bestmodel = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLOY02Vi2YiB"
      },
      "source": [
        "# Step2:\n",
        "## Load the dataset and do initial data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LfOMKc9l2YiC"
      },
      "outputs": [],
      "source": [
        "#load the dataset\n",
        "faceRecog_data = np.load('ORL_faces.npz') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnN-1I_f2YiC",
        "outputId": "bf14c2c7-1cf9-4597-89aa-1d2fa970622f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 762412\n",
            "-rw-r--r-- 1 root root 747721704 Mar 13 15:59 FaceRecog.h5\n",
            "-rw-r--r-- 1 root root  32973922 Mar 13 15:16 ORL_faces.npz\n",
            "drwxr-xr-x 1 root root      4096 Mar  9 14:48 sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YFo4YGEE2YiD"
      },
      "outputs": [],
      "source": [
        "# load the \"Train Images\"\n",
        "X_train = faceRecog_data['trainX']\n",
        "\n",
        "X_test = faceRecog_data['testX']\n",
        "\n",
        "\n",
        "#normalize every image \n",
        "# Note, the format of the image is a Uint8 matrix of pixels. We convert to float or double\n",
        "X_train = np.array(X_train,dtype='float64')/255\n",
        "X_test = np.array(X_test,dtype='float64')/255\n",
        "\n",
        "# load the Label of Images\n",
        "y_train= faceRecog_data['trainY']\n",
        "y_test= faceRecog_data['testY']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNPwEZ5u2YiE",
        "outputId": "0bef8fb8-c642-454e-c5c3-b459f7121f5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train : [[0.18823529 0.19215686 0.17647059 ... 0.18431373 0.18039216 0.18039216]\n",
            " [0.23529412 0.23529412 0.24313725 ... 0.1254902  0.13333333 0.13333333]\n",
            " [0.15294118 0.17254902 0.20784314 ... 0.11372549 0.10196078 0.11372549]\n",
            " ...\n",
            " [0.44705882 0.45882353 0.44705882 ... 0.38431373 0.37647059 0.38431373]\n",
            " [0.41176471 0.41176471 0.41960784 ... 0.21176471 0.18431373 0.16078431]\n",
            " [0.45490196 0.44705882 0.45882353 ... 0.37254902 0.39215686 0.39607843]]\n",
            "X_train shape: (240, 10304)\n",
            "X_test shape: (160, 10304)\n",
            "y_train shape: (240,)\n"
          ]
        }
      ],
      "source": [
        "# View the train and test data format\n",
        "print('X_train : {}'.format(X_train[:]))\n",
        "print('X_train shape: {}'.format(X_train.shape))\n",
        "print('X_test shape: {}'.format(X_test.shape))\n",
        "print('y_train shape: {}'.format(y_train.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aax2Mrxv2YiF"
      },
      "source": [
        "# Step 3\n",
        "\n",
        "### Split that dataSet into validation and train datasets \n",
        "\n",
        "**The Validation dataset is used to reduce any potential overfitting. As a general rule of thumb, if the accuracy of the training dataset increases, but the accuracy of the validation dataset stays the same or diminishes significantly, then you're probably overfitting.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9zFAkGXe2YiF"
      },
      "outputs": [],
      "source": [
        "# Here, we will use 10% for validation.\n",
        "X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size=.10, random_state=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3t9TF2P2YiG"
      },
      "source": [
        "# Step 4\n",
        "\n",
        "### We need to setup the data for the CNN. To do achieve this, we rescale the size of images to be the same"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9zwy3IZv2YiG"
      },
      "outputs": [],
      "source": [
        "im_rows=112\n",
        "im_cols=92\n",
        "batch_size=512\n",
        "im_shape=(im_rows, im_cols, 1)\n",
        "\n",
        "#change the size of images\n",
        "X_train = X_train.reshape(X_train.shape[0], *im_shape)\n",
        "X_test = X_test.reshape(X_test.shape[0], *im_shape)\n",
        "X_validate = X_validate.reshape(X_validate.shape[0], *im_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iiitZLJ2YiG",
        "outputId": "435349c6-2d9e-45b0-c454-6b028314d237"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (216, 112, 92, 1)\n",
            "x_test shape: (160,)\n"
          ]
        }
      ],
      "source": [
        "# Check the shape\n",
        "print('x_train shape: {}'.format(X_train.shape))\n",
        "print('x_test shape: {}'.format(y_test.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL3UzUym2YiH"
      },
      "source": [
        "# Step 5\n",
        "\n",
        "\n",
        "## Build the CNN model:\n",
        " **CNN have 3 main layer** \n",
        " * 1-Convolotional layer \n",
        " * 2- pooling layer  \n",
        " * 3- fully connected layer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7Ok4lOUj2YiH"
      },
      "outputs": [],
      "source": [
        "\n",
        "#filters= the depth of output image or kernels\n",
        "lr = 2e-4\n",
        "decay = 6e-8\n",
        "CNN_model= Sequential([\n",
        "    Conv2D(filters=36, kernel_size=5, activation='relu', input_shape= im_shape),\n",
        "    MaxPooling2D(pool_size=2),\n",
        "    Conv2D(filters=54, kernel_size=3, activation='relu', input_shape= im_shape),\n",
        "    MaxPooling2D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(2024, activation='relu'),\n",
        "     Dropout(0.3),\n",
        "    Dense(1024, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(20, activation='softmax')  \n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "CNN_model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=Adam(learning_rate=lr*0.5, decay=decay*0.5),\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6IAMbu82YiI",
        "outputId": "c3af7773-d581-4adf-9d62-f210a91759e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 108, 88, 36)       936       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 54, 44, 36)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 52, 42, 54)        17550     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 26, 21, 54)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 29484)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2024)              59677640  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 2024)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1024)              2073600   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               524800    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 20)                10260     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 62,304,786\n",
            "Trainable params: 62,304,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# View model parameters\n",
        "CNN_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2olYMkY-2YiI"
      },
      "source": [
        "# Step 6\n",
        "\n",
        "Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PnUZx_U2YiI",
        "outputId": "0d65f477-edaf-45e8-a74b-82c30281e5f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 3.0021 - accuracy: 0.0469\n",
            "Epoch 1: val_accuracy improved from inf to 0.04167, saving model to FaceRecog.h5\n",
            "1/1 [==============================] - 10s 10s/step - loss: 3.0021 - accuracy: 0.0469 - val_loss: 2.9764 - val_accuracy: 0.0417\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 3.0279 - accuracy: 0.0000e+00\n",
            "Epoch 2: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 1s 674ms/step - loss: 3.0279 - accuracy: 0.0000e+00 - val_loss: 2.9873 - val_accuracy: 0.0417\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.9738 - accuracy: 0.0938\n",
            "Epoch 3: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 2.9738 - accuracy: 0.0938 - val_loss: 3.0001 - val_accuracy: 0.0417\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.9838 - accuracy: 0.0568\n",
            "Epoch 4: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 2.9838 - accuracy: 0.0568 - val_loss: 3.0027 - val_accuracy: 0.0833\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.9890 - accuracy: 0.0469\n",
            "Epoch 5: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 2.9890 - accuracy: 0.0469 - val_loss: 2.9979 - val_accuracy: 0.0833\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.9621 - accuracy: 0.1250\n",
            "Epoch 6: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 2.9621 - accuracy: 0.1250 - val_loss: 2.9971 - val_accuracy: 0.0417\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.9633 - accuracy: 0.0625\n",
            "Epoch 7: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 2.9633 - accuracy: 0.0625 - val_loss: 2.9940 - val_accuracy: 0.0417\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.9842 - accuracy: 0.0682\n",
            "Epoch 8: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 2.9842 - accuracy: 0.0682 - val_loss: 2.9881 - val_accuracy: 0.0417\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.9431 - accuracy: 0.0859\n",
            "Epoch 9: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 2.9431 - accuracy: 0.0859 - val_loss: 2.9822 - val_accuracy: 0.0417\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.9418 - accuracy: 0.1136\n",
            "Epoch 10: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 347ms/step - loss: 2.9418 - accuracy: 0.1136 - val_loss: 2.9692 - val_accuracy: 0.0417\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.9564 - accuracy: 0.0938\n",
            "Epoch 11: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 375ms/step - loss: 2.9564 - accuracy: 0.0938 - val_loss: 2.9585 - val_accuracy: 0.0833\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.9356 - accuracy: 0.0455\n",
            "Epoch 12: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 267ms/step - loss: 2.9356 - accuracy: 0.0455 - val_loss: 2.9422 - val_accuracy: 0.1250\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.9009 - accuracy: 0.1406\n",
            "Epoch 13: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 315ms/step - loss: 2.9009 - accuracy: 0.1406 - val_loss: 2.9252 - val_accuracy: 0.1250\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.8956 - accuracy: 0.1250\n",
            "Epoch 14: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 415ms/step - loss: 2.8956 - accuracy: 0.1250 - val_loss: 2.9069 - val_accuracy: 0.1250\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.9028 - accuracy: 0.1484\n",
            "Epoch 15: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 418ms/step - loss: 2.9028 - accuracy: 0.1484 - val_loss: 2.8884 - val_accuracy: 0.1667\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.8644 - accuracy: 0.1932\n",
            "Epoch 16: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 363ms/step - loss: 2.8644 - accuracy: 0.1932 - val_loss: 2.8704 - val_accuracy: 0.2500\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.8502 - accuracy: 0.1406\n",
            "Epoch 17: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 410ms/step - loss: 2.8502 - accuracy: 0.1406 - val_loss: 2.8567 - val_accuracy: 0.2917\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.8265 - accuracy: 0.1932\n",
            "Epoch 18: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 424ms/step - loss: 2.8265 - accuracy: 0.1932 - val_loss: 2.8337 - val_accuracy: 0.2500\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.8482 - accuracy: 0.1641\n",
            "Epoch 19: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 329ms/step - loss: 2.8482 - accuracy: 0.1641 - val_loss: 2.8076 - val_accuracy: 0.2917\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.7535 - accuracy: 0.2727\n",
            "Epoch 20: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 326ms/step - loss: 2.7535 - accuracy: 0.2727 - val_loss: 2.7850 - val_accuracy: 0.3750\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.7991 - accuracy: 0.1797\n",
            "Epoch 21: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 389ms/step - loss: 2.7991 - accuracy: 0.1797 - val_loss: 2.7628 - val_accuracy: 0.4583\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.7359 - accuracy: 0.2273\n",
            "Epoch 22: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 2.7359 - accuracy: 0.2273 - val_loss: 2.7364 - val_accuracy: 0.3750\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.7614 - accuracy: 0.2344\n",
            "Epoch 23: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 2.7614 - accuracy: 0.2344 - val_loss: 2.7121 - val_accuracy: 0.3333\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.7489 - accuracy: 0.2159\n",
            "Epoch 24: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 2.7489 - accuracy: 0.2159 - val_loss: 2.6834 - val_accuracy: 0.3750\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.6674 - accuracy: 0.2188\n",
            "Epoch 25: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 2.6674 - accuracy: 0.2188 - val_loss: 2.6512 - val_accuracy: 0.3333\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.6415 - accuracy: 0.2614\n",
            "Epoch 26: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 2.6415 - accuracy: 0.2614 - val_loss: 2.6210 - val_accuracy: 0.3333\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.5745 - accuracy: 0.3438\n",
            "Epoch 27: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 2.5745 - accuracy: 0.3438 - val_loss: 2.5873 - val_accuracy: 0.4583\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.6723 - accuracy: 0.2386\n",
            "Epoch 28: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 2.6723 - accuracy: 0.2386 - val_loss: 2.5477 - val_accuracy: 0.4583\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.5794 - accuracy: 0.2500\n",
            "Epoch 29: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 2.5794 - accuracy: 0.2500 - val_loss: 2.4973 - val_accuracy: 0.5417\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.5672 - accuracy: 0.2727\n",
            "Epoch 30: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 2.5672 - accuracy: 0.2727 - val_loss: 2.4422 - val_accuracy: 0.6250\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3871 - accuracy: 0.3828\n",
            "Epoch 31: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 2.3871 - accuracy: 0.3828 - val_loss: 2.3855 - val_accuracy: 0.6667\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.5003 - accuracy: 0.3409\n",
            "Epoch 32: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 2.5003 - accuracy: 0.3409 - val_loss: 2.3207 - val_accuracy: 0.7917\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3862 - accuracy: 0.3359\n",
            "Epoch 33: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 2.3862 - accuracy: 0.3359 - val_loss: 2.2609 - val_accuracy: 0.8333\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3586 - accuracy: 0.3523\n",
            "Epoch 34: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 2.3586 - accuracy: 0.3523 - val_loss: 2.2048 - val_accuracy: 0.9167\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3465 - accuracy: 0.3594\n",
            "Epoch 35: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 2.3465 - accuracy: 0.3594 - val_loss: 2.1511 - val_accuracy: 0.8750\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.2876 - accuracy: 0.4205\n",
            "Epoch 36: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 2.2876 - accuracy: 0.4205 - val_loss: 2.0999 - val_accuracy: 0.8333\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.1997 - accuracy: 0.4609\n",
            "Epoch 37: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 2.1997 - accuracy: 0.4609 - val_loss: 2.0550 - val_accuracy: 0.8333\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.0789 - accuracy: 0.5000\n",
            "Epoch 38: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 2.0789 - accuracy: 0.5000 - val_loss: 1.9923 - val_accuracy: 0.7917\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.0851 - accuracy: 0.4609\n",
            "Epoch 39: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 2.0851 - accuracy: 0.4609 - val_loss: 1.9248 - val_accuracy: 0.7917\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.0136 - accuracy: 0.5568\n",
            "Epoch 40: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 2.0136 - accuracy: 0.5568 - val_loss: 1.8520 - val_accuracy: 0.8333\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.0007 - accuracy: 0.4531\n",
            "Epoch 41: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 2.0007 - accuracy: 0.4531 - val_loss: 1.7710 - val_accuracy: 0.8333\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.0863 - accuracy: 0.3523\n",
            "Epoch 42: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 2.0863 - accuracy: 0.3523 - val_loss: 1.6999 - val_accuracy: 0.9167\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.8724 - accuracy: 0.5547\n",
            "Epoch 43: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 1.8724 - accuracy: 0.5547 - val_loss: 1.6277 - val_accuracy: 0.9583\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.8509 - accuracy: 0.5341\n",
            "Epoch 44: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 1.8509 - accuracy: 0.5341 - val_loss: 1.5662 - val_accuracy: 0.9583\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.7239 - accuracy: 0.5000\n",
            "Epoch 45: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 1.7239 - accuracy: 0.5000 - val_loss: 1.4868 - val_accuracy: 1.0000\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.7905 - accuracy: 0.5568\n",
            "Epoch 46: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 1.7905 - accuracy: 0.5568 - val_loss: 1.4421 - val_accuracy: 0.9167\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.6681 - accuracy: 0.5938\n",
            "Epoch 47: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 1.6681 - accuracy: 0.5938 - val_loss: 1.3843 - val_accuracy: 0.9167\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.5426 - accuracy: 0.6591\n",
            "Epoch 48: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 1.5426 - accuracy: 0.6591 - val_loss: 1.3195 - val_accuracy: 0.9167\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.5628 - accuracy: 0.5547\n",
            "Epoch 49: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 1.5628 - accuracy: 0.5547 - val_loss: 1.2644 - val_accuracy: 1.0000\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.5727 - accuracy: 0.5909\n",
            "Epoch 50: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 1.5727 - accuracy: 0.5909 - val_loss: 1.1930 - val_accuracy: 1.0000\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.5216 - accuracy: 0.5938\n",
            "Epoch 51: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 1.5216 - accuracy: 0.5938 - val_loss: 1.1088 - val_accuracy: 1.0000\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.3781 - accuracy: 0.6364\n",
            "Epoch 52: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 1.3781 - accuracy: 0.6364 - val_loss: 1.0428 - val_accuracy: 1.0000\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.3148 - accuracy: 0.7109\n",
            "Epoch 53: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 1.3148 - accuracy: 0.7109 - val_loss: 0.9854 - val_accuracy: 1.0000\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.2569 - accuracy: 0.6818\n",
            "Epoch 54: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 1.2569 - accuracy: 0.6818 - val_loss: 0.9468 - val_accuracy: 1.0000\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.2760 - accuracy: 0.6328\n",
            "Epoch 55: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 1.2760 - accuracy: 0.6328 - val_loss: 0.9085 - val_accuracy: 1.0000\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.0333 - accuracy: 0.8068\n",
            "Epoch 56: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 1.0333 - accuracy: 0.8068 - val_loss: 0.8610 - val_accuracy: 1.0000\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.1280 - accuracy: 0.6562\n",
            "Epoch 57: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 1.1280 - accuracy: 0.6562 - val_loss: 0.8112 - val_accuracy: 1.0000\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.1462 - accuracy: 0.7273\n",
            "Epoch 58: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 1.1462 - accuracy: 0.7273 - val_loss: 0.7750 - val_accuracy: 1.0000\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.0052 - accuracy: 0.7578\n",
            "Epoch 59: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 1.0052 - accuracy: 0.7578 - val_loss: 0.7076 - val_accuracy: 1.0000\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.8967 - accuracy: 0.7955\n",
            "Epoch 60: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.8967 - accuracy: 0.7955 - val_loss: 0.6762 - val_accuracy: 1.0000\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.9143 - accuracy: 0.7344\n",
            "Epoch 61: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.9143 - accuracy: 0.7344 - val_loss: 0.6594 - val_accuracy: 0.9583\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.8922 - accuracy: 0.7500\n",
            "Epoch 62: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.8922 - accuracy: 0.7500 - val_loss: 0.6274 - val_accuracy: 1.0000\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7913 - accuracy: 0.8047\n",
            "Epoch 63: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.7913 - accuracy: 0.8047 - val_loss: 0.5856 - val_accuracy: 1.0000\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7810 - accuracy: 0.7614\n",
            "Epoch 64: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.7810 - accuracy: 0.7614 - val_loss: 0.5274 - val_accuracy: 1.0000\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6424 - accuracy: 0.9062\n",
            "Epoch 65: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.6424 - accuracy: 0.9062 - val_loss: 0.4761 - val_accuracy: 1.0000\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.8462 - accuracy: 0.7955\n",
            "Epoch 66: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.8462 - accuracy: 0.7955 - val_loss: 0.4391 - val_accuracy: 1.0000\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7068 - accuracy: 0.8047\n",
            "Epoch 67: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.7068 - accuracy: 0.8047 - val_loss: 0.4170 - val_accuracy: 1.0000\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5968 - accuracy: 0.8750\n",
            "Epoch 68: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.5968 - accuracy: 0.8750 - val_loss: 0.4070 - val_accuracy: 1.0000\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5948 - accuracy: 0.8828\n",
            "Epoch 69: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.5948 - accuracy: 0.8828 - val_loss: 0.4020 - val_accuracy: 1.0000\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6751 - accuracy: 0.7841\n",
            "Epoch 70: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.6751 - accuracy: 0.7841 - val_loss: 0.3847 - val_accuracy: 1.0000\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5957 - accuracy: 0.8359\n",
            "Epoch 71: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.5957 - accuracy: 0.8359 - val_loss: 0.3425 - val_accuracy: 1.0000\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5267 - accuracy: 0.8977\n",
            "Epoch 72: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.5267 - accuracy: 0.8977 - val_loss: 0.3188 - val_accuracy: 1.0000\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5411 - accuracy: 0.8750\n",
            "Epoch 73: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.5411 - accuracy: 0.8750 - val_loss: 0.3026 - val_accuracy: 1.0000\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4854 - accuracy: 0.8977\n",
            "Epoch 74: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.4854 - accuracy: 0.8977 - val_loss: 0.2697 - val_accuracy: 1.0000\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5308 - accuracy: 0.8594\n",
            "Epoch 75: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.5308 - accuracy: 0.8594 - val_loss: 0.2440 - val_accuracy: 1.0000\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4473 - accuracy: 0.8977\n",
            "Epoch 76: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.4473 - accuracy: 0.8977 - val_loss: 0.2281 - val_accuracy: 1.0000\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3839 - accuracy: 0.9297\n",
            "Epoch 77: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 0.3839 - accuracy: 0.9297 - val_loss: 0.2253 - val_accuracy: 1.0000\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5133 - accuracy: 0.8636\n",
            "Epoch 78: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.5133 - accuracy: 0.8636 - val_loss: 0.2077 - val_accuracy: 1.0000\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3338 - accuracy: 0.9375\n",
            "Epoch 79: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.3338 - accuracy: 0.9375 - val_loss: 0.2049 - val_accuracy: 1.0000\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3787 - accuracy: 0.9205\n",
            "Epoch 80: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.3787 - accuracy: 0.9205 - val_loss: 0.1717 - val_accuracy: 1.0000\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3921 - accuracy: 0.8750\n",
            "Epoch 81: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.3921 - accuracy: 0.8750 - val_loss: 0.1457 - val_accuracy: 1.0000\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3156 - accuracy: 0.9432\n",
            "Epoch 82: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.3156 - accuracy: 0.9432 - val_loss: 0.1283 - val_accuracy: 1.0000\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3653 - accuracy: 0.9219\n",
            "Epoch 83: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.3653 - accuracy: 0.9219 - val_loss: 0.1234 - val_accuracy: 1.0000\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3370 - accuracy: 0.9318\n",
            "Epoch 84: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 0.3370 - accuracy: 0.9318 - val_loss: 0.1334 - val_accuracy: 1.0000\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3185 - accuracy: 0.9141\n",
            "Epoch 85: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.3185 - accuracy: 0.9141 - val_loss: 0.1601 - val_accuracy: 1.0000\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2485 - accuracy: 0.9432\n",
            "Epoch 86: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 0.2485 - accuracy: 0.9432 - val_loss: 0.2026 - val_accuracy: 0.9583\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2702 - accuracy: 0.9609\n",
            "Epoch 87: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.2702 - accuracy: 0.9609 - val_loss: 0.2172 - val_accuracy: 0.9583\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2012 - accuracy: 0.9886\n",
            "Epoch 88: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.2012 - accuracy: 0.9886 - val_loss: 0.2031 - val_accuracy: 0.9583\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2278 - accuracy: 0.9766\n",
            "Epoch 89: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.2278 - accuracy: 0.9766 - val_loss: 0.1710 - val_accuracy: 0.9583\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2795 - accuracy: 0.9545\n",
            "Epoch 90: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.2795 - accuracy: 0.9545 - val_loss: 0.1237 - val_accuracy: 1.0000\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2675 - accuracy: 0.9297\n",
            "Epoch 91: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.2675 - accuracy: 0.9297 - val_loss: 0.0937 - val_accuracy: 1.0000\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1665 - accuracy: 0.9886\n",
            "Epoch 92: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.1665 - accuracy: 0.9886 - val_loss: 0.0756 - val_accuracy: 1.0000\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1806 - accuracy: 0.9766\n",
            "Epoch 93: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.1806 - accuracy: 0.9766 - val_loss: 0.0714 - val_accuracy: 1.0000\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1836 - accuracy: 0.9773\n",
            "Epoch 94: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.1836 - accuracy: 0.9773 - val_loss: 0.0684 - val_accuracy: 1.0000\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2438 - accuracy: 0.9297\n",
            "Epoch 95: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.2438 - accuracy: 0.9297 - val_loss: 0.0722 - val_accuracy: 1.0000\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1486 - accuracy: 0.9659\n",
            "Epoch 96: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.1486 - accuracy: 0.9659 - val_loss: 0.0807 - val_accuracy: 1.0000\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1299 - accuracy: 0.9922\n",
            "Epoch 97: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.1299 - accuracy: 0.9922 - val_loss: 0.0930 - val_accuracy: 0.9583\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1422 - accuracy: 0.9773\n",
            "Epoch 98: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 0.1422 - accuracy: 0.9773 - val_loss: 0.1053 - val_accuracy: 0.9583\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1767 - accuracy: 0.9688\n",
            "Epoch 99: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.1767 - accuracy: 0.9688 - val_loss: 0.1042 - val_accuracy: 0.9583\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1048 - accuracy: 0.9886\n",
            "Epoch 100: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.1048 - accuracy: 0.9886 - val_loss: 0.1005 - val_accuracy: 0.9583\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1189 - accuracy: 0.9844\n",
            "Epoch 101: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.1189 - accuracy: 0.9844 - val_loss: 0.0888 - val_accuracy: 1.0000\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1117 - accuracy: 0.9773\n",
            "Epoch 102: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.1117 - accuracy: 0.9773 - val_loss: 0.0763 - val_accuracy: 1.0000\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1372 - accuracy: 0.9688\n",
            "Epoch 103: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.1372 - accuracy: 0.9688 - val_loss: 0.0561 - val_accuracy: 1.0000\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1277 - accuracy: 0.9773\n",
            "Epoch 104: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.1277 - accuracy: 0.9773 - val_loss: 0.0462 - val_accuracy: 1.0000\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1001 - accuracy: 0.9766\n",
            "Epoch 105: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.1001 - accuracy: 0.9766 - val_loss: 0.0408 - val_accuracy: 1.0000\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1116 - accuracy: 0.9886\n",
            "Epoch 106: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.1116 - accuracy: 0.9886 - val_loss: 0.0353 - val_accuracy: 1.0000\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1088 - accuracy: 0.9922\n",
            "Epoch 107: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.1088 - accuracy: 0.9922 - val_loss: 0.0300 - val_accuracy: 1.0000\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0524 - accuracy: 1.0000\n",
            "Epoch 108: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.0524 - accuracy: 1.0000 - val_loss: 0.0258 - val_accuracy: 1.0000\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1129 - accuracy: 0.9688\n",
            "Epoch 109: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 0.1129 - accuracy: 0.9688 - val_loss: 0.0232 - val_accuracy: 1.0000\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1347 - accuracy: 0.9773\n",
            "Epoch 110: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 0.1347 - accuracy: 0.9773 - val_loss: 0.0234 - val_accuracy: 1.0000\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9922\n",
            "Epoch 111: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.0711 - accuracy: 0.9922 - val_loss: 0.0253 - val_accuracy: 1.0000\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0834 - accuracy: 0.9886\n",
            "Epoch 112: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 0.0834 - accuracy: 0.9886 - val_loss: 0.0279 - val_accuracy: 1.0000\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0643 - accuracy: 1.0000\n",
            "Epoch 113: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.0643 - accuracy: 1.0000 - val_loss: 0.0291 - val_accuracy: 1.0000\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 1.0000\n",
            "Epoch 114: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.0730 - accuracy: 1.0000 - val_loss: 0.0281 - val_accuracy: 1.0000\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0481 - accuracy: 1.0000\n",
            "Epoch 115: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.0481 - accuracy: 1.0000 - val_loss: 0.0281 - val_accuracy: 1.0000\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0827 - accuracy: 1.0000\n",
            "Epoch 116: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 0.0827 - accuracy: 1.0000 - val_loss: 0.0311 - val_accuracy: 1.0000\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0579 - accuracy: 1.0000\n",
            "Epoch 117: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.0579 - accuracy: 1.0000 - val_loss: 0.0368 - val_accuracy: 1.0000\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0808 - accuracy: 0.9886\n",
            "Epoch 118: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 0.0808 - accuracy: 0.9886 - val_loss: 0.0379 - val_accuracy: 1.0000\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9922\n",
            "Epoch 119: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.0692 - accuracy: 0.9922 - val_loss: 0.0390 - val_accuracy: 1.0000\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0653 - accuracy: 0.9886\n",
            "Epoch 120: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 0.0653 - accuracy: 0.9886 - val_loss: 0.0344 - val_accuracy: 1.0000\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0463 - accuracy: 0.9922\n",
            "Epoch 121: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.0463 - accuracy: 0.9922 - val_loss: 0.0294 - val_accuracy: 1.0000\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 1.0000\n",
            "Epoch 122: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 0.0717 - accuracy: 1.0000 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9766\n",
            "Epoch 123: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.0706 - accuracy: 0.9766 - val_loss: 0.0196 - val_accuracy: 1.0000\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 1.0000\n",
            "Epoch 124: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.0755 - accuracy: 1.0000 - val_loss: 0.0171 - val_accuracy: 1.0000\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0850 - accuracy: 0.9844\n",
            "Epoch 125: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.0850 - accuracy: 0.9844 - val_loss: 0.0167 - val_accuracy: 1.0000\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9886\n",
            "Epoch 126: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.0335 - accuracy: 0.9886 - val_loss: 0.0193 - val_accuracy: 1.0000\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0468 - accuracy: 0.9922\n",
            "Epoch 127: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.0468 - accuracy: 0.9922 - val_loss: 0.0232 - val_accuracy: 1.0000\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9886\n",
            "Epoch 128: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 0.0726 - accuracy: 0.9886 - val_loss: 0.0243 - val_accuracy: 1.0000\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0488 - accuracy: 0.9922\n",
            "Epoch 129: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.0488 - accuracy: 0.9922 - val_loss: 0.0228 - val_accuracy: 1.0000\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 1.0000\n",
            "Epoch 130: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 0.0385 - accuracy: 1.0000 - val_loss: 0.0199 - val_accuracy: 1.0000\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 1.0000\n",
            "Epoch 131: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 0.0381 - accuracy: 1.0000 - val_loss: 0.0165 - val_accuracy: 1.0000\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 1.0000\n",
            "Epoch 132: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.0239 - accuracy: 1.0000 - val_loss: 0.0159 - val_accuracy: 1.0000\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 1.0000\n",
            "Epoch 133: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.0292 - accuracy: 1.0000 - val_loss: 0.0158 - val_accuracy: 1.0000\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0571 - accuracy: 0.9886\n",
            "Epoch 134: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.0571 - accuracy: 0.9886 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 1.0000\n",
            "Epoch 135: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.0358 - accuracy: 1.0000 - val_loss: 0.0142 - val_accuracy: 1.0000\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 1.0000\n",
            "Epoch 136: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.0289 - accuracy: 1.0000 - val_loss: 0.0151 - val_accuracy: 1.0000\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0393 - accuracy: 1.0000\n",
            "Epoch 137: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.0393 - accuracy: 1.0000 - val_loss: 0.0185 - val_accuracy: 1.0000\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 1.0000\n",
            "Epoch 138: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.0331 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 1.0000\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 1.0000\n",
            "Epoch 139: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.0459 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 1.0000\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 1.0000\n",
            "Epoch 140: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 0.0358 - accuracy: 1.0000 - val_loss: 0.0186 - val_accuracy: 1.0000\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 1.0000\n",
            "Epoch 141: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 0.0382 - accuracy: 1.0000 - val_loss: 0.0130 - val_accuracy: 1.0000\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 1.0000\n",
            "Epoch 142: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.0223 - accuracy: 1.0000 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 1.0000\n",
            "Epoch 143: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.0247 - accuracy: 1.0000 - val_loss: 0.0118 - val_accuracy: 1.0000\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0460 - accuracy: 1.0000\n",
            "Epoch 144: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 0.0460 - accuracy: 1.0000 - val_loss: 0.0134 - val_accuracy: 1.0000\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 1.0000\n",
            "Epoch 145: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.0333 - accuracy: 1.0000 - val_loss: 0.0149 - val_accuracy: 1.0000\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0364 - accuracy: 1.0000\n",
            "Epoch 146: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.0364 - accuracy: 1.0000 - val_loss: 0.0168 - val_accuracy: 1.0000\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 1.0000\n",
            "Epoch 147: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.0200 - val_accuracy: 1.0000\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 1.0000\n",
            "Epoch 148: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.0244 - accuracy: 1.0000 - val_loss: 0.0216 - val_accuracy: 1.0000\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 1.0000\n",
            "Epoch 149: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.0202 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 1.0000\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 1.0000\n",
            "Epoch 150: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 0.0365 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 1.0000\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 1.0000\n",
            "Epoch 151: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.0216 - accuracy: 1.0000 - val_loss: 0.0204 - val_accuracy: 1.0000\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 1.0000\n",
            "Epoch 152: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 0.0227 - accuracy: 1.0000 - val_loss: 0.0178 - val_accuracy: 1.0000\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 1.0000\n",
            "Epoch 153: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 0.0156 - val_accuracy: 1.0000\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 1.0000\n",
            "Epoch 154: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0463 - accuracy: 0.9844\n",
            "Epoch 155: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 0.0463 - accuracy: 0.9844 - val_loss: 0.0114 - val_accuracy: 1.0000\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 1.0000\n",
            "Epoch 156: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.0210 - accuracy: 1.0000 - val_loss: 0.0097 - val_accuracy: 1.0000\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 1.0000\n",
            "Epoch 157: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.0084 - val_accuracy: 1.0000\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9886\n",
            "Epoch 158: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.0268 - accuracy: 0.9886 - val_loss: 0.0076 - val_accuracy: 1.0000\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 1.0000\n",
            "Epoch 159: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 1.0000\n",
            "Epoch 160: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 0.0275 - accuracy: 1.0000 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9922\n",
            "Epoch 161: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.0203 - accuracy: 0.9922 - val_loss: 0.0078 - val_accuracy: 1.0000\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 1.0000\n",
            "Epoch 162: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.0168 - accuracy: 1.0000 - val_loss: 0.0091 - val_accuracy: 1.0000\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9922\n",
            "Epoch 163: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.0334 - accuracy: 0.9922 - val_loss: 0.0093 - val_accuracy: 1.0000\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 1.0000\n",
            "Epoch 164: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 0.0185 - accuracy: 1.0000 - val_loss: 0.0085 - val_accuracy: 1.0000\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9922\n",
            "Epoch 165: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.0313 - accuracy: 0.9922 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9886\n",
            "Epoch 166: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 0.0241 - accuracy: 0.9886 - val_loss: 0.0096 - val_accuracy: 1.0000\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 1.0000\n",
            "Epoch 167: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.0119 - val_accuracy: 1.0000\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 1.0000\n",
            "Epoch 168: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.0261 - accuracy: 1.0000 - val_loss: 0.0150 - val_accuracy: 1.0000\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 1.0000\n",
            "Epoch 169: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.0185 - val_accuracy: 1.0000\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 1.0000\n",
            "Epoch 170: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.0227 - accuracy: 1.0000 - val_loss: 0.0175 - val_accuracy: 1.0000\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0115 - accuracy: 1.0000\n",
            "Epoch 171: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.0168 - val_accuracy: 1.0000\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 1.0000\n",
            "Epoch 172: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.0153 - val_accuracy: 1.0000\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9922\n",
            "Epoch 173: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.0248 - accuracy: 0.9922 - val_loss: 0.0129 - val_accuracy: 1.0000\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 1.0000\n",
            "Epoch 174: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 1.0000\n",
            "Epoch 175: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.0219 - accuracy: 1.0000 - val_loss: 0.0075 - val_accuracy: 1.0000\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 1.0000\n",
            "Epoch 176: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.0215 - accuracy: 1.0000 - val_loss: 0.0057 - val_accuracy: 1.0000\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0106 - accuracy: 1.0000\n",
            "Epoch 177: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9886\n",
            "Epoch 178: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.0216 - accuracy: 0.9886 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 1.0000\n",
            "Epoch 179: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.0192 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 1.0000\n",
            "Epoch 180: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 1.0000\n",
            "Epoch 181: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.0157 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 1.0000\n",
            "Epoch 182: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0093 - accuracy: 1.0000\n",
            "Epoch 183: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 1.0000\n",
            "Epoch 184: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 1.0000\n",
            "Epoch 185: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 1.0000\n",
            "Epoch 186: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 1.0000\n",
            "Epoch 187: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0099 - accuracy: 1.0000\n",
            "Epoch 188: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 1.0000\n",
            "Epoch 189: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 1.0000\n",
            "Epoch 190: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0063 - accuracy: 1.0000\n",
            "Epoch 191: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0055 - val_accuracy: 1.0000\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 1.0000\n",
            "Epoch 192: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.0073 - val_accuracy: 1.0000\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0093 - accuracy: 1.0000\n",
            "Epoch 193: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.0087 - val_accuracy: 1.0000\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 1.0000\n",
            "Epoch 194: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.0097 - val_accuracy: 1.0000\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 1.0000\n",
            "Epoch 195: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.0118 - val_accuracy: 1.0000\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 1.0000\n",
            "Epoch 196: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 0.0152 - val_accuracy: 1.0000\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 1.0000\n",
            "Epoch 197: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0186 - val_accuracy: 1.0000\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 1.0000\n",
            "Epoch 198: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.0214 - val_accuracy: 1.0000\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 1.0000\n",
            "Epoch 199: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.0217 - val_accuracy: 1.0000\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 1.0000\n",
            "Epoch 200: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.0202 - val_accuracy: 1.0000\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 1.0000\n",
            "Epoch 201: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.0168 - val_accuracy: 1.0000\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 1.0000\n",
            "Epoch 202: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0071 - accuracy: 1.0000\n",
            "Epoch 203: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0098 - val_accuracy: 1.0000\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 1.0000\n",
            "Epoch 204: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 0.0180 - accuracy: 1.0000 - val_loss: 0.0075 - val_accuracy: 1.0000\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 1.0000\n",
            "Epoch 205: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0062 - val_accuracy: 1.0000\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0097 - accuracy: 1.0000\n",
            "Epoch 206: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0074 - accuracy: 1.0000\n",
            "Epoch 207: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0059 - accuracy: 1.0000\n",
            "Epoch 208: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 1.0000\n",
            "Epoch 209: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0057 - accuracy: 1.0000\n",
            "Epoch 210: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9922\n",
            "Epoch 211: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.0181 - accuracy: 0.9922 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0068 - accuracy: 1.0000\n",
            "Epoch 212: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0105 - accuracy: 1.0000\n",
            "Epoch 213: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0104 - accuracy: 1.0000\n",
            "Epoch 214: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 1.0000\n",
            "Epoch 215: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0067 - accuracy: 1.0000\n",
            "Epoch 216: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0077 - accuracy: 1.0000\n",
            "Epoch 217: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0058 - accuracy: 1.0000\n",
            "Epoch 218: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0071 - accuracy: 1.0000\n",
            "Epoch 219: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 1.0000\n",
            "Epoch 220: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 1.0000\n",
            "Epoch 221: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 1.0000\n",
            "Epoch 222: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.0055 - val_accuracy: 1.0000\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0105 - accuracy: 1.0000\n",
            "Epoch 223: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.0055 - val_accuracy: 1.0000\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 224: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0057 - val_accuracy: 1.0000\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 1.0000\n",
            "Epoch 225: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0060 - accuracy: 1.0000\n",
            "Epoch 226: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0064 - val_accuracy: 1.0000\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0061 - accuracy: 1.0000\n",
            "Epoch 227: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0065 - val_accuracy: 1.0000\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0059 - accuracy: 1.0000\n",
            "Epoch 228: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0058 - val_accuracy: 1.0000\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 1.0000\n",
            "Epoch 229: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9886\n",
            "Epoch 230: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.0328 - accuracy: 0.9886 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 231: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0064 - accuracy: 1.0000\n",
            "Epoch 232: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0051 - accuracy: 1.0000\n",
            "Epoch 233: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0076 - accuracy: 1.0000\n",
            "Epoch 234: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.0064 - val_accuracy: 1.0000\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0060 - accuracy: 1.0000\n",
            "Epoch 235: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0085 - val_accuracy: 1.0000\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 1.0000\n",
            "Epoch 236: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.0103 - val_accuracy: 1.0000\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 237: val_accuracy did not improve from 0.04167\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0105 - val_accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# Define model callbacks\n",
        "model_callbacks = [keras.callbacks.ModelCheckpoint('FaceRecog.h5',monitor='val_accuracy',save_best_only=True,mode='min',verbose=1),\n",
        "                   keras.callbacks.EarlyStopping(monitor='val_loss',patience=25,mode='min',min_delta=0.0001,restore_best_weights=True)\n",
        "    \n",
        "    \n",
        "]\n",
        "\n",
        "batch_size = 128\n",
        "history=CNN_model.fit(X_train, y_train, \n",
        "                      batch_size=batch_size,\n",
        "                      epochs=500,\n",
        "                      steps_per_epoch = X_train.shape[0]//batch_size,\n",
        "                      validation_data=(X_validate, y_validate),\n",
        "                      #validation_steps = X_validate.shape[0]//batch_size, #Data too small for this to be meaningful\n",
        "                      callbacks = [model_callbacks],\n",
        "                      verbose=1\n",
        "                     )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taNC5Ncs2YiJ"
      },
      "source": [
        "## Evaluate the accuracy of the model. We use the test data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "104yF4122YiJ",
        "outputId": "b8e2c29e-c5f1-40ce-85bf-27b24be58c0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 20ms/step - loss: 0.2688 - accuracy: 0.9375\n",
            "test los 0.2688\n",
            "test acc 0.9375\n"
          ]
        }
      ],
      "source": [
        "test_score = CNN_model.evaluate(X_test, y_test)\n",
        "\n",
        "print('test los {:.4f}'.format(test_score[0]))\n",
        "print('test acc {:.4f}'.format(test_score[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCHK5RMr2YiJ"
      },
      "source": [
        "# Step 7 \n",
        "\n",
        "plot and visualize some of the key results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqvZQQWp2YiK",
        "outputId": "e7ddd928-16c2-4d41-bf6a-0662139bbb30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ]
        }
      ],
      "source": [
        "print(history.history.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "s7elO_B72YiK",
        "outputId": "6c9a5401-66ea-4741-bae8-5894bb3d00d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         loss  accuracy  val_loss  val_accuracy  epoch\n",
              "232  0.005138       1.0  0.004679           1.0    232\n",
              "233  0.007558       1.0  0.006377           1.0    233\n",
              "234  0.006017       1.0  0.008465           1.0    234\n",
              "235  0.010076       1.0  0.010347           1.0    235\n",
              "236  0.005558       1.0  0.010492           1.0    236"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-03bfc8ad-0b38-4d29-b4eb-c9b583a7ba92\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_accuracy</th>\n",
              "      <th>epoch</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>232</th>\n",
              "      <td>0.005138</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.004679</td>\n",
              "      <td>1.0</td>\n",
              "      <td>232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233</th>\n",
              "      <td>0.007558</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.006377</td>\n",
              "      <td>1.0</td>\n",
              "      <td>233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234</th>\n",
              "      <td>0.006017</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.008465</td>\n",
              "      <td>1.0</td>\n",
              "      <td>234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>235</th>\n",
              "      <td>0.010076</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.010347</td>\n",
              "      <td>1.0</td>\n",
              "      <td>235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>236</th>\n",
              "      <td>0.005558</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.010492</td>\n",
              "      <td>1.0</td>\n",
              "      <td>236</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-03bfc8ad-0b38-4d29-b4eb-c9b583a7ba92')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-03bfc8ad-0b38-4d29-b4eb-c9b583a7ba92 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-03bfc8ad-0b38-4d29-b4eb-c9b583a7ba92');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "hist.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "DqxIyZPs2YiK",
        "outputId": "196798b3-2e6d-4687-d249-30bb6e201f82"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "wErfd-mZ2YiK",
        "outputId": "d5813196-9916-4e9e-a100-ec3c39f661c8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1Htj_Hi2YiK"
      },
      "source": [
        "# Step 8\n",
        "\n",
        "Plot the Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svyS_pT02YiL",
        "outputId": "01dc382a-8056-4ad8-df34-65ba95c0c0a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy is :  0.9375\n"
          ]
        }
      ],
      "source": [
        "predicted = CNN_model.predict(X_test)# (CNN_model.predict(X_test)> 0.5).astype(\"int32\")\n",
        "ynew = np.argmax(predicted,axis=1) #CNN_model.predict_classes(X_test)\n",
        "\n",
        "\n",
        "Accuracy = accuracy_score(y_test, ynew)\n",
        "print(\"The accuracy is : \",Accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RE_JeqKl2YiL"
      },
      "outputs": [],
      "source": [
        "# Define a function to plot the confusion matrix\n",
        "#/tn, fp, fn, tp = confusion_matrix(np.array(y_test), ynew).ravel()\n",
        "cnf_matrix = confusion_matrix(np.array(y_test), ynew)\n",
        "\n",
        "y_test1 = np_utils.to_categorical(y_test, 20)\n",
        "\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=True,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        #print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm)\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLM16Cfd2YiL",
        "outputId": "4125a182-abff-421f-8bad-42ae6a27c730"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix, with normalization\n",
            "[[8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 6 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
            " [0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 2 0 6 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0]\n",
            " [2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 2 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 6 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8]]\n"
          ]
        }
      ],
      "source": [
        "print('Confusion Matrix, with normalization')\n",
        "print(cnf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IdraJ4_32YiL",
        "outputId": "c54478ec-efda-4ee0-e271-ed966ad68f80"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 6 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
            " [0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 2 0 6 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0]\n",
            " [2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 2 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 6 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      1.00      0.89         8\n",
            "           1       1.00      1.00      1.00         8\n",
            "           2       1.00      1.00      1.00         8\n",
            "           3       1.00      1.00      1.00         8\n",
            "           4       1.00      0.75      0.86         8\n",
            "           5       1.00      1.00      1.00         8\n",
            "           6       1.00      1.00      1.00         8\n",
            "           7       0.80      1.00      0.89         8\n",
            "           8       1.00      1.00      1.00         8\n",
            "           9       1.00      0.75      0.86         8\n",
            "          10       0.80      1.00      0.89         8\n",
            "          11       1.00      1.00      1.00         8\n",
            "          12       1.00      1.00      1.00         8\n",
            "          13       1.00      1.00      1.00         8\n",
            "          14       1.00      1.00      1.00         8\n",
            "          15       1.00      0.50      0.67         8\n",
            "          16       0.80      1.00      0.89         8\n",
            "          17       0.80      1.00      0.89         8\n",
            "          18       1.00      0.75      0.86         8\n",
            "          19       1.00      1.00      1.00         8\n",
            "\n",
            "    accuracy                           0.94       160\n",
            "   macro avg       0.95      0.94      0.93       160\n",
            "weighted avg       0.95      0.94      0.93       160\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Plot the CM and visualize it\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix[1:10,1:10], classes=[0,1,2,3,4,5,6,7,8,9],\n",
        "                      title='Normalized Confusion Matrix')\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix[11:20,11:20], classes=[10,11,12,13,14,15,16,17,18,19],\n",
        "                      title='Normalized Confusion Matrix')\n",
        "\n",
        "print(\"Confusion Matrix:\\n%s\" % confusion_matrix(np.array(y_test), ynew))\n",
        "print(classification_report(np.array(y_test), ynew))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy8E-hM_2YiM"
      },
      "source": [
        "# Step 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DjjC8eo2YiM"
      },
      "source": [
        "# We can use the saved best model to do predictions, especially in the future\n",
        "**You need GPU for this. Better Amazon SageMaker. Note, this is not an advertisement**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_WO8B4P2YiM",
        "outputId": "dbf66751-0968-4c13-ae16-4a2f478763d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 108, 88, 36)       936       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 54, 44, 36)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 52, 42, 54)        17550     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 26, 21, 54)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 29484)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2024)              59677640  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 2024)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1024)              2073600   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               524800    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 20)                10260     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 62,304,786\n",
            "Trainable params: 62,304,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "The accuracy is :  0.9375\n"
          ]
        }
      ],
      "source": [
        "if bestmodel==True:\n",
        "    best_model = keras.models.load_model('FaceRecog.h5',compile=True)\n",
        "    best_model.summary()\n",
        "best_model.predict(X_test)\n",
        "ynew = np.argmax(predicted,axis=1)\n",
        "#Print the accuracy\n",
        "print(\"The accuracy is : \",accuracy_score(y_test, ynew))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End of Case Study"
      ],
      "metadata": {
        "id": "O28zK6jfB4aO"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "CNN_Face_recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}